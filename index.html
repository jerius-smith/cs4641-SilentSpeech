<!DOCTYPE html>
<html lang="en">
<head>
    <title>CS 4641 - Silent Speech</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="index.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"></script>
</head>
<header>
    <div class="container-fluid p-3 my-3 bg-dark text-white">
        <h1>Silent Speller</h1>
        <p>CS 4641 Spring 2020</p>
    </div>
</header>
<body>
  <div class="info">
    <h2>Introduction</h2>
     <p>We created a silent spelling system that aims to recognize language from silent utterances. This system is useful for people
         with speech impairments, for people in noisy environments, and for people in social situations that restrict speech. Speech
         impairments may result from ALS, tracheostomies, or deafness. Noisy environments limit the effectivess of technologies that rely on speech as input, such as conversational AI and dictation tools.
         Social situations, like meetings, and private information may require a subtler interface than speech. Researchers have explored using different sensors, like
         surface electromyography and video, to capture data that encodes speech. We further existing research on speech recognition
         by using Hidden Markov Models to recognize silent utterances at the letter level.
     </p>

      <h2>Related Works</h2>
      <p>
      In 1989, Steve Young developed the HTK Hidden Markov Model Toolkit at Cambridge University for the primary purpose of speech recognition research. Interest in the software developed from other speech laboratories and universities for pedagogical purposes. By 1994, Young had described in detail the design and philosophy supporting HTK.
      </p>
      <p>
      By 2003, the burgeoning field of wearable computing saw increased use of gesture recognition as a tool for interaction. However, building systems for gesture recognition required significant knowledge and effort. Westeyn developed the Georgia Tech Gesture Toolkit on top of HTK to ease the development of such a system. The Georgia Tech Gesture Toolkit trains models that recognize in real-time and off-line.
      </p>
      <p>
      More recently, a silent speech recognition system was developed by Li. The system relied on the CompleteSpeech SmartPalate to collect data and a support vector machine to recognize speech from the collected data. The system had a 21 word vocabulary, and was evaluated through user studies with offline and online recognition for native and non-native users. Li also examined interaction with the silent spelling system in two settings: sitting and walking. Their results indicated that the system used by native speakers had high accuracy and information transfer rate comparable to that of a mouse and touchscreen.
      </p>
      <div class="figures">
          <div class="figure">
              <img src="./res/SmartPalate.png" height="450" width="450">
              <p>Figure 1: SmartPalate Device</p>
          </div>
      </div>
      <p>


      </p>

      <

    </div>
    <div class="info">
        <h2>Experimental Design</h2>
        <p>In an effort to develop our silent speller system, we trained a Hidden Markov Model to recognize silent utterances of letters using a dataset collected by Naoki Kimura and optimized our pipeline with respect to speed and memory.<br></p>


        <h4>Data collection</h4>

       <p>     The CompleteSpeech SmartPalate is a retainer fitted with 124 capacitive sensors. Each sensor records a zero or one at a frequency
            of 100Hz. Naoki used the SmartPalate to collect the dataset used to train and test our Hidden Markov Models. The dataset consisted of 20 samples
            of each letter of the alphabet. Each sample was collected over the course of a second, so each sample consists of 100 frames of a 124 diminsional binary vector.</p>

        <div class="figures">
            <div class="inlineFigurefigure">
                <img src="./res/A1.png" height="288" width="432">
                <p>Figure 2-1: Visualization of the capacitive sensors that were activated over the course of uttering the letter "A"</p>
            </div>

            <div class="inlineigure">
                <img src="./res/AllSensors.png" height="288" width="432">
                <p>Figure 2-2: Visualization of the 128 capacitive sensors in the SmartPalate.</p>
            </div>

            <div class="inlineFigurefigure">
                <img src="./res/B1.png" height="288" width="432">
                <p>Figure 2-3: Visualization of the capacitive sensors that were activated over the course of uttering the letter "B"</p>
            </div>
        </div>

        <h4>Data Processing</h4>

        <p>
            The algorithm used to classify the first dataset was a Hidden Markov Model. Specifically, we used a left-to-right Hidden Markov Model, which results in a HMM representation like in figure_:





            To optimize our silent spelling system's speed, we subsampled the data. Subsamples of the data were created by summing multiple timesteps into a single timestep or grouping the capacitive sensors into sections. To subsample over time, we summed every five consecutive 128 dimensional vectors to a single 128 dimensional vector with values between 0 and 5. Subsampling by section converted each 128 dimensional vector into a 10 dimensional vector with values representing the amount of capacitive sensors activated in each group.
            <div class="figures">
            <div class="inlineFigurefigure">
                <img src="./res/AllSensorsGrouped.png" height="288" width="432">
                <p>Figure 2-4: Visualization of the capacitive sensors that were activated over the course of uttering the letter "B"</p>
            </div>
  <div class="inlineFigurefigure">
      <img src="./res/subsampling.svg" height="600" width="600">
      <p>Figure 2-5: </p>
  </div>

</div>
        </p>

    </div>
    <div class="info">
        <h2></h2>
        <p></p>
    </div>
    <div class="info">
        <h2>Evaluation</h2>
        <p>
            To evaluate our recognizer's accuracy, we performed 4-fold stratified cross validation. The overall accuracy of the model was 79.82%.
        </p>
        <img
            src="./res/confusionMatrix.svg"
            alt="Confusion Matrix"
            height="800"
            width="800" />

        <h2>Conclusion and Future Work</h2>
        <p>The accuracy of our models were determined by its offline classification performance.</p>
        <p>Moving forward, we plan on increasing the accuracy of our recognizer
            by introducing context training and a statistical grammar, measures that have been shown to decrease the error rate by a factor of eight in practice.</p>

        <h2>References</h2>
        <ol>

            <li>R. Lee, J Wu, and T. Starner. TongueBoard: An Oral Interface for Subtle Input. In Proceedings of the 10th Augmented Human International Conference 2019. ACM, New York, NY, USA.</li>
            <li>N. Kimura, M. Kono, and J. Rekimoto. 2019. SottoVoce: An Ultrasound Imaging-Based Silent Speech Interaction Using Deep Neural Networks. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI â€˜19). ACM, New York, NY, USA, 1 -- 11.</li>
            <li>Jelinek, F.; Bahl, L.; Mercer, R. (1975). "Design of a linguistic statistical decoder for the recognition of continuous speech". IEEE Transactions on Information Theory. 21 (3): 250.</li>
            <li>S.J. Young and Sj Young. The HTK Hidden Markov Model Toolkit: Design and Philosophy. Entropic Cambridge Research Laboratory, Ltd 1994. 2--44.</li>
            <li>Xuedong Huang; M. Jack; Y. Ariki (1990). Hidden Markov Models for Speech Recognition. Edinburgh University Press. ISBN 978-0-7486-0162-2.</li>
            <li>B. Denby, T. Schultz, K. Honda, T. Hueber, J. M. Gilbert, and J. S. Brumberg. 2010. Silent Speech Interfaces. Speech Commun. 52, 4 (April 2010), 270--287.</li>
            <li>Diandra Fabre, Thomas Hueber, Laurent Girin, Xavier Alameda-Pineda, and Pierre Badin. 2017. Automatic animation of an articulatory tongue model from ultrasound images of the vocal tract. Speech Communication 93 (2017), 63 -- 75.</li>
            <li>M.J. Fagan, S.R. Ell, J.M. Gilbert, E. Sarrazin, and P.M. Chapman. 2008. Development of a (silent) speech recognition system for patients following laryngectomy. Medical Engineering & Physics 30, 4 (2008), 419 -- 425.</li>
            <li>Masaaki Fukumoto. 2018. SilentVoice: Unnoticeable Voice Input by Ingressive Speech. In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18). ACM, New York, NY, USA, 237--246.</li>
            <li>MacKenzie, I. S., & Soukoreff, R. W. (2003). Phrase sets for evaluating text entry techniques. Extended Abstracts of the ACM Conference on Human Factors in Computing Systems - CHI 2003, pp. 754-755. New York: ACM.</li>

        </ol>
    </div>
</body>
</html>
